#!/usr/bin/env python3
"""
Dataset Preprocessing Configuration

Centralized path configuration for all concept normalization dataset preprocessing scripts.
Users should modify the paths below to match their local setup.
"""

from pathlib import Path

# =============================================================================
# BASE DIRECTORIES - MODIFY THESE TO MATCH YOUR SETUP
# =============================================================================

# Root directory of this project (automatically detected)
PROJECT_ROOT = Path(__file__).parent.resolve()

# =============================================================================
# UMLS DATA PATHS (from umls_preprocessing module)
# =============================================================================

# Path to UMLS preprocessing output directory
# # This should point to the output folder from the umls_preprocessing module
# UMLS_OUTPUT_DIR = PROJECT_ROOT.parent / "umls_preprocessing" / "output"

# # Core UMLS files needed by dataset preprocessing
# # These are generated by the umls_preprocessing scripts
# SEMANTIC_MAPPING_FILE = UMLS_OUTPUT_DIR / "cui_semantic_mapping.json"

# =============================================================================
# ADDITIONAL UMLS FILES (user must provide these)
# =============================================================================

# Directory containing additional UMLS-derived mapping files
UMLS_MAPPINGS_DIR = PROJECT_ROOT / "data" / "mappings"

SEMANTIC_MAPPING_FILE = UMLS_MAPPINGS_DIR / "cui_semantic_mapping.json"


# Word to ontology mapping file (maps terms to source vocabularies)
# Format: {"term": ["MSH", "SNOMEDCT_US", ...], ...}
WORD_ONTOLOGY_MAP_FILE = UMLS_MAPPINGS_DIR / "word_mapping_onto.json"

# CUI codes list file (list of valid UMLS CUIs)
# Format: ["C0000001", "C0000002", ...]
CUI_CODES_FILE = UMLS_MAPPINGS_DIR / "cui_codes.json"

# Source vocabulary to UMLS CUI mapping files
MESH_MAP_FILE = UMLS_MAPPINGS_DIR / "mesh_map.json"
SNOMED_MAP_FILE = UMLS_MAPPINGS_DIR / "snomed_map.json"
ATC_MAP_FILE = UMLS_MAPPINGS_DIR / "atc_map.json"
ICD10_MAP_FILE = UMLS_MAPPINGS_DIR / "icd10_map.json"
LOINC_MAP_FILE = UMLS_MAPPINGS_DIR / "loinc_map.json"

# =============================================================================
# DATASET SOURCE DIRECTORIES
# =============================================================================

# Base directory for all raw dataset files
DATASETS_BASE_DIR = PROJECT_ROOT / "data" / "datasets"

# Individual dataset directories (each should have a source_files/ subfolder)
BC5CDR_DIR = DATASETS_BASE_DIR / "BC5CDR"
N2C2_DIR = DATASETS_BASE_DIR / "N2C2"
QUAERO_DIR = DATASETS_BASE_DIR / "Quaero"
MED_UCD_DIR = DATASETS_BASE_DIR / "MedUCD"
DISTEMIST_DIR = DATASETS_BASE_DIR / "DisTEMIST"
PHARMA_DIR = DATASETS_BASE_DIR / "Pharma"
BRONCO_DIR = DATASETS_BASE_DIR / "BRONCO150"
TLLV_DIR = DATASETS_BASE_DIR / "TLLV"
GSC_DIR = DATASETS_BASE_DIR / "GSC"
XL_BEL_DIR = DATASETS_BASE_DIR / "XL-BEL"

# =============================================================================
# OUTPUT DIRECTORIES
# =============================================================================

# Base output directory
OUTPUT_BASE_DIR = PROJECT_ROOT / "output"

# Combined output directory (all datasets merged by split)
ALL_DATASETS_OUTPUT_DIR = OUTPUT_BASE_DIR / "all"
TRAIN_OUTPUT_DIR = ALL_DATASETS_OUTPUT_DIR / "train"
DEV_OUTPUT_DIR = ALL_DATASETS_OUTPUT_DIR / "dev"
TEST_OUTPUT_DIR = ALL_DATASETS_OUTPUT_DIR / "test"

# Per-dataset output directory
BY_DATASET_OUTPUT_DIR = OUTPUT_BASE_DIR / "by_dataset"

# =============================================================================
# LANGUAGE CODES
# =============================================================================

LANGUAGE_CODES = {
    'en': 'eng',
    'de': 'ger', 
    'es': 'spa',
    'fr': 'fre',
    'tr': 'tur',
    'pt': 'por',
    'it': 'ita',
    'nl': 'dut',
}

# =============================================================================
# CSV OUTPUT COLUMNS
# =============================================================================

CSV_COLUMNS = [
    'term',
    'code', 
    'langauge',  # Note: keeping original spelling for compatibility
    'semantic_type',
    'semantic_group',
    'targe_ontologies',  # Note: keeping original spelling for compatibility
    'exact_match',
    'source'
]

# =============================================================================
# HELPER FUNCTIONS
# =============================================================================

def ensure_directories():
    """Create all output directories if they don't exist."""
    directories = [
        OUTPUT_BASE_DIR,
        ALL_DATASETS_OUTPUT_DIR,
        TRAIN_OUTPUT_DIR,
        DEV_OUTPUT_DIR,
        TEST_OUTPUT_DIR,
        BY_DATASET_OUTPUT_DIR,
        UMLS_MAPPINGS_DIR,
        DATASETS_BASE_DIR,
    ]
    for directory in directories:
        directory.mkdir(parents=True, exist_ok=True)


def ensure_dataset_output_dir(dataset_name: str) -> Path:
    """Create and return output directory for a specific dataset."""
    output_dir = BY_DATASET_OUTPUT_DIR / dataset_name / "processed"
    output_dir.mkdir(parents=True, exist_ok=True)
    return output_dir


def validate_file(filepath: Path, description: str) -> bool:
    """Check if a file exists and print helpful message if not."""
    if not filepath.exists():
        print(f"WARNING: {description} not found at:")
        print(f"         {filepath}")
        return False
    return True


def validate_umls_files() -> bool:
    """Validate that required UMLS files exist."""
    files_to_check = [
        (SEMANTIC_MAPPING_FILE, "Semantic mapping file (from umls_preprocessing)"),
        (WORD_ONTOLOGY_MAP_FILE, "Word-to-ontology mapping file"),
        (CUI_CODES_FILE, "CUI codes file"),
    ]
    
    all_valid = True
    for filepath, description in files_to_check:
        if not validate_file(filepath, description):
            all_valid = False
    
    return all_valid


def get_config_summary() -> str:
    """Return a summary of current configuration."""
    
    def check_mark(path):
        return '✓' if path.exists() else '✗'
    
    return f"""
Dataset Preprocessing Configuration
====================================
Project Root:          {PROJECT_ROOT}
Datasets Base Dir:     {DATASETS_BASE_DIR}
Output Base Dir:       {OUTPUT_BASE_DIR}

Required UMLS Files:
  Semantic Mapping:    {SEMANTIC_MAPPING_FILE} {check_mark(SEMANTIC_MAPPING_FILE)}
  Word-Ontology Map:   {WORD_ONTOLOGY_MAP_FILE} {check_mark(WORD_ONTOLOGY_MAP_FILE)}
  CUI Codes:           {CUI_CODES_FILE} {check_mark(CUI_CODES_FILE)}

Mapping Files:
  MeSH Map:            {MESH_MAP_FILE} {check_mark(MESH_MAP_FILE)}
  SNOMED Map:          {SNOMED_MAP_FILE} {check_mark(SNOMED_MAP_FILE)}
  ATC Map:             {ATC_MAP_FILE} {check_mark(ATC_MAP_FILE)}
  ICD-10 Map:          {ICD10_MAP_FILE} {check_mark(ICD10_MAP_FILE)}
  LOINC Map:           {LOINC_MAP_FILE} {check_mark(LOINC_MAP_FILE)}

Dataset Directories:
  BC5CDR:              {BC5CDR_DIR} {check_mark(BC5CDR_DIR)}
  N2C2:                {N2C2_DIR} {check_mark(N2C2_DIR)}
  Quaero:              {QUAERO_DIR} {check_mark(QUAERO_DIR)}
  MedUCD:              {MED_UCD_DIR} {check_mark(MED_UCD_DIR)}
  DisTEMIST:           {DISTEMIST_DIR} {check_mark(DISTEMIST_DIR)}
  Pharma:              {PHARMA_DIR} {check_mark(PHARMA_DIR)}
  BRONCO150:           {BRONCO_DIR} {check_mark(BRONCO_DIR)}
  TLLV:             {TLLV_DIR} {check_mark(TLLV_DIR)}
  GSC:                 {GSC_DIR} {check_mark(GSC_DIR)}
  XL-BEL:              {XL_BEL_DIR} {check_mark(XL_BEL_DIR)}
"""


if __name__ == "__main__":
    print(get_config_summary())
